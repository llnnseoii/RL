"""
PPO Variants with Actual Training - Conservative, Balanced, Aggressive

ê° variantë³„ë¡œ PPOë¥¼ ì‹¤ì œë¡œ í•™ìŠµì‹œí‚¨ í›„ episode ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from typing import List, Dict
import pickle

from parameters import SimulationParameters
from output import Output
from entities import Person


# ==================== PPO Network (drl_optimizerì™€ ë™ì¼) ====================
class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(ActorCritic, self).__init__()
        
        # Actor
        self.actor = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim),
            nn.Softmax(dim=-1)
        )
        
        # Critic
        self.critic = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )
    
    def forward(self):
        raise NotImplementedError
    
    def act(self, state):
        state = torch.from_numpy(state).float()
        action_probs = self.actor(state)
        dist = torch.distributions.Categorical(action_probs)
        action = dist.sample()
        return action.item(), dist.log_prob(action)
    
    def evaluate(self, state, action):
        action_probs = self.actor(state)
        dist = torch.distributions.Categorical(action_probs)
        action_logprobs = dist.log_prob(action)
        dist_entropy = dist.entropy()
        state_value = self.critic(state)
        return action_logprobs, torch.squeeze(state_value), dist_entropy


# ==================== Simplified Environment ====================
class ScreeningEnv:
    """ê°„ì†Œí™”ëœ ê²€ì§„ í™˜ê²½ (PPO í•™ìŠµìš©)"""
    
    def __init__(self, params, variant='balanced'):
        self.params = params
        self.variant = variant
        self.min_age = params.opt_min_age
        self.max_age = params.opt_max_age
        self.state_dim = 16
        self.max_history = 10
        
        # Variantë³„ reward íŒŒë¼ë¯¸í„°
        if variant == 'conservative':
            self.cost_penalty = 0.10
            self.survival_reward = 0.1
            self.cancer_bonus = 0.0
        elif variant == 'balanced':
            self.cost_penalty = 0.05
            self.survival_reward = 0.1
            self.cancer_bonus = 0.0
        else:  # aggressive
            self.cost_penalty = 0.02
            self.survival_reward = 0.15
            self.cancer_bonus = 2.0
        
        self.reset()
    
    def reset(self):
        """ìƒˆë¡œìš´ personìœ¼ë¡œ ë¦¬ì…‹"""
        self.rng = np.random.default_rng()
        self.person = Person(self.params, self.rng)
        self.person.simulate()
        
        self.output = Output(self.params)
        self.current_age = self.min_age
        self.years_since_last = self.max_history
        
        return self._get_state()
    
    def _get_state(self):
        """State vector ìƒì„±"""
        age_norm = self.current_age / 100.0
        hist_norm = self.years_since_last / 10.0
        gender = float(self.person.gender)
        zones = np.zeros(13)
        
        adv_threshold = getattr(self.params, 'adv_polyp_transition', 5)
        for p in self.person.polyps:
            if p.age_developed <= self.current_age < p.age_end:
                loc = min(p.location, 12)
                if p.stage >= adv_threshold:
                    zones[loc] = 2
                else:
                    zones[loc] = 1
        
        for c in self.person.cancers:
            if c.age_developed <= self.current_age:
                loc = min(c.location, 12)
                if c.detected:
                    zones[loc] = 4
                else:
                    zones[loc] = 3
        
        return np.array([age_norm, hist_norm, gender] + list(zones), dtype=np.float32)
    
    def step(self, action):
        """Action ìˆ˜í–‰"""
        reward = 0.0
        
        if action == 1:  # Screen
            prev_cost = self.output.total_discounted_cost
            self.person._perform_colonoscopy(self.current_age, self.output, is_screening=True)
            step_cost = self.output.total_discounted_cost - prev_cost
            
            reward -= (step_cost * 0.0001 + self.cost_penalty)
            
            # Aggressive: ì•” ë°œê²¬ ë³´ë„ˆìŠ¤
            if self.variant == 'aggressive' and self.output.screen_detections > 0:
                reward += self.cancer_bonus
            
            self.years_since_last = 0
        else:
            self.years_since_last = min(self.years_since_last + 1, self.max_history)
        
        # ë‚˜ì´ ì¦ê°€
        self.current_age += 1
        done = False
        
        if self.current_age >= self.person.death_age:
            reward -= 1.0
            done = True
        elif self.current_age >= self.max_age:
            reward += 1.0
            done = True
        else:
            reward += self.survival_reward
        
        return self._get_state(), reward, done


# ==================== PPO Trainer ====================
class PPOTrainer:
    """PPO í•™ìŠµê¸°"""
    
    def __init__(self, params, variant='balanced'):
        self.params = params
        self.variant = variant
        self.env = ScreeningEnv(params, variant)
        
        self.state_dim = 16
        self.action_dim = 2
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Networks
        self.policy = ActorCritic(self.state_dim, self.action_dim).to(self.device)
        self.policy_old = ActorCritic(self.state_dim, self.action_dim).to(self.device)
        self.policy_old.load_state_dict(self.policy.state_dict())
        
        self.optimizer = optim.Adam(self.policy.parameters(), lr=3e-4)
        
        # Hyperparameters
        self.gamma = 0.99
        self.K_epochs = 4
        self.eps_clip = 0.2
        self.update_timestep = 2000
    
    def train(self, max_episodes=1000):
        """PPO í•™ìŠµ"""
        print(f"\nğŸ“ Training {self.variant.upper()} PPO for {max_episodes} episodes...")
        
        timestep = 0
        memory_states = []
        memory_actions = []
        memory_logprobs = []
        memory_rewards = []
        memory_is_terminals = []
        
        for ep in range(1, max_episodes + 1):
            state = self.env.reset()
            ep_reward = 0
            
            while True:
                timestep += 1
                
                # Action ì„ íƒ
                action, log_prob = self.policy_old.act(state)
                
                # í™˜ê²½ ì§„í–‰
                next_state, reward, done = self.env.step(action)
                
                # ë©”ëª¨ë¦¬ ì €ì¥
                memory_states.append(torch.from_numpy(state).float())
                memory_actions.append(torch.tensor(action))
                memory_logprobs.append(log_prob)
                memory_rewards.append(reward)
                memory_is_terminals.append(done)
                
                state = next_state
                ep_reward += reward
                
                # ì—…ë°ì´íŠ¸
                if timestep % self.update_timestep == 0:
                    self.update(memory_states, memory_actions, memory_logprobs, 
                               memory_rewards, memory_is_terminals)
                    memory_states, memory_actions, memory_logprobs = [], [], []
                    memory_rewards, memory_is_terminals = [], []
                
                if done:
                    break
            
            if ep % 100 == 0:
                print(f"  Episode {ep}/{max_episodes}: Reward = {ep_reward:.2f}")
        
        print(f"âœ… {self.variant.upper()} PPO Training Complete!")
        return self.policy
    
    def update(self, states, actions, logprobs, rewards, is_terminals):
        """PPO ì—…ë°ì´íŠ¸"""
        # Discount rewards
        discounted_rewards = []
        discounted_reward = 0
        for reward, is_terminal in zip(reversed(rewards), reversed(is_terminals)):
            if is_terminal:
                discounted_reward = 0
            discounted_reward = reward + (self.gamma * discounted_reward)
            discounted_rewards.insert(0, discounted_reward)
        
        discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)
        if len(discounted_rewards) > 1:
            discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-5)
        
        old_states = torch.stack(states)
        old_actions = torch.stack(actions)
        old_logprobs = torch.stack(logprobs).detach()
        
        # ì—¬ëŸ¬ epoch ì—…ë°ì´íŠ¸
        for _ in range(self.K_epochs):
            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)
            
            ratios = torch.exp(logprobs - old_logprobs)
            advantages = discounted_rewards - state_values.detach()
            
            surr1 = ratios * advantages
            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages
            
            loss = -torch.min(surr1, surr2) + 0.5 * F.mse_loss(state_values, discounted_rewards) - 0.01 * dist_entropy
            
            self.optimizer.zero_grad()
            loss.mean().backward()
            self.optimizer.step()
        
        self.policy_old.load_state_dict(self.policy.state_dict())
    
    def collect_episodes(self, n_episodes=1000):
        """í•™ìŠµëœ policyë¡œ episode ìˆ˜ì§‘"""
        print(f"\nğŸ“Š Collecting {n_episodes} episodes with trained {self.variant.upper()} policy...")
        
        episodes = []
        
        for ep_idx in range(n_episodes):
            episode_data = self._run_episode()
            episodes.append(episode_data)
            
            if (ep_idx + 1) % 100 == 0:
                avg_reward = np.mean([e['total_reward'] for e in episodes[-100:]])
                print(f"  [{ep_idx+1}/{n_episodes}] Avg Reward: {avg_reward:.2f}")
        
        print(f"âœ… {len(episodes)} episodes collected!")
        return episodes
    
    def _run_episode(self):
        """ë‹¨ì¼ episode ì‹¤í–‰ ë° ë°ì´í„° ìˆ˜ì§‘"""
        state = self.env.reset()
        
        transitions = []
        total_reward = 0.0
        num_screenings = 0
        cancer_detected = False
        
        while True:
            # Trained policyë¡œ action ì„ íƒ
            action, _ = self.policy_old.act(state)
            
            # Step
            next_state, reward, done = self.env.step(action)
            
            if action == 1:
                num_screenings += 1
                if self.env.output.screen_detections > 0:
                    cancer_detected = True
            
            # Transition ì €ì¥
            transitions.append({
                'state': state,
                'action': action,
                'reward': reward,
                'next_state': next_state,
                'done': done
            })
            
            total_reward += reward
            state = next_state
            
            if done:
                break
        
        # Episode summary
        life_years_gained = min(
            self.env.current_age - self.env.min_age,
            self.env.person.death_age - self.env.min_age
        )
        
        return {
            'total_reward': total_reward,
            'life_years_gained': life_years_gained,
            'total_cost': self.env.output.total_discounted_cost,
            'cancer_detected': cancer_detected,
            'num_screenings': num_screenings,
            'transitions': transitions,
            'variant': self.variant
        }


# ==================== Main Function ====================
def train_and_collect_all_variants(params, episodes_per_variant=1000, 
                                   training_episodes=1000, save_path='diverse_ppo_data.pkl'):
    """
    3ê°€ì§€ PPO variant í•™ìŠµ í›„ episode ìˆ˜ì§‘
    
    Args:
        params: SimulationParameters
        episodes_per_variant: ìˆ˜ì§‘í•  episode ìˆ˜
        training_episodes: PPO í•™ìŠµ episode ìˆ˜
        save_path: ì €ì¥ ê²½ë¡œ
    
    Returns:
        ì „ì²´ episode ë¦¬ìŠ¤íŠ¸
    """
    all_episodes = []
    
    for variant in ['conservative', 'balanced', 'aggressive']:
        print(f"\n{'='*60}")
        print(f"  Variant: {variant.upper()}")
        print(f"{'='*60}")
        
        # PPO í•™ìŠµ
        trainer = PPOTrainer(params, variant=variant)
        trainer.train(max_episodes=training_episodes)
        
        # Episode ìˆ˜ì§‘
        episodes = trainer.collect_episodes(n_episodes=episodes_per_variant)
        all_episodes.extend(episodes)
        
        # ëª¨ë¸ ì €ì¥ (ì„ íƒì‚¬í•­)
        torch.save(trainer.policy.state_dict(), f'ppo_{variant}.pth')
        print(f"ğŸ’¾ Model saved: ppo_{variant}.pth")
    
    # ì „ì²´ ë°ì´í„° ì €ì¥
    with open(save_path, 'wb') as f:
        pickle.dump(all_episodes, f)
    
    print(f"\n{'='*60}")
    print(f"âœ… ì „ì²´ {len(all_episodes)} episodes ì €ì¥: {save_path}")
    print(f"{'='*60}")
    print(f"  Conservative: {sum(1 for e in all_episodes if e['variant'] == 'conservative')}")
    print(f"  Balanced: {sum(1 for e in all_episodes if e['variant'] == 'balanced')}")
    print(f"  Aggressive: {sum(1 for e in all_episodes if e['variant'] == 'aggressive')}")
    print(f"{'='*60}")
    
    return all_episodes


if __name__ == "__main__":
    from parameters import SimulationParameters
    
    params = SimulationParameters('settings.ini')
    
    # 3ê°€ì§€ variant í•™ìŠµ ë° ìˆ˜ì§‘
    dataset = train_and_collect_all_variants(
        params,
        episodes_per_variant=1000,    # ìˆ˜ì§‘í•  episodes
        training_episodes=1000,        # í•™ìŠµ episodes
        save_path='diverse_ppo_data.pkl'
    )

    """ë‹¤ì–‘í•œ PPO ì •ì±…ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘"""
    
    def __init__(self, params, variant='balanced'):
        """
        Args:
            variant: 'conservative', 'balanced', 'aggressive'
        """
        self.params = params
        self.variant = variant
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Variantë³„ reward ì„¤ì •
        if variant == 'conservative':
            self.cost_penalty = 0.10  # ë†’ì€ ë¹„ìš© í˜ë„í‹°
            self.survival_reward = 0.1
            self.cancer_detection_bonus = 0.0
            print("ğŸŸ¦ Conservative Policy: ë¹„ìš© ì¤‘ì‹œ, ê²€ì§„ ìµœì†Œí™”")
            
        elif variant == 'balanced':
            self.cost_penalty = 0.05  # í‘œì¤€
            self.survival_reward = 0.1
            self.cancer_detection_bonus = 0.0
            print("ğŸŸ© Balanced Policy: í‘œì¤€ ê°€ì´ë“œë¼ì¸")
            
        elif variant == 'aggressive':
            self.cost_penalty = 0.02  # ë‚®ì€ ë¹„ìš© í˜ë„í‹°
            self.survival_reward = 0.15  # ë†’ì€ ìƒì¡´ ë³´ìƒ
            self.cancer_detection_bonus = 2.0  # ì•” ë°œê²¬ ë³´ìƒ
            print("ğŸŸ¥ Aggressive Policy: ìƒì¡´ ì¤‘ì‹œ, ì¡°ê¸° ë°œê²¬")
        
        self.min_age = params.opt_min_age
        self.max_age = params.opt_max_age
        self.state_dim = 16
        self.max_history = 10
    
    def collect_episodes(self, n_episodes=1000, rng_seed=42) -> List[Dict]:
        """
        ì§€ì •ëœ ìˆ˜ì˜ episode ë°ì´í„° ìˆ˜ì§‘
        
        Returns:
            List of episode data with quality metrics
        """
        episodes = []
        rng = np.random.default_rng(rng_seed)
        
        print(f"\nğŸ“Š {self.variant.upper()} Policyë¡œ {n_episodes} episodes ìˆ˜ì§‘ ì¤‘...")
        
        for ep_idx in range(n_episodes):
            episode_data = self._run_episode(rng)
            episodes.append(episode_data)
            
            if (ep_idx + 1) % 100 == 0:
                avg_reward = np.mean([e['total_reward'] for e in episodes[-100:]])
                print(f"  [{ep_idx+1}/{n_episodes}] Avg Reward: {avg_reward:.2f}")
        
        print(f"âœ… {len(episodes)} episodes ìˆ˜ì§‘ ì™„ë£Œ!")
        return episodes
    
    def _run_episode(self, rng) -> Dict:
        """ë‹¨ì¼ episode ì‹¤í–‰ ë° ë°ì´í„° ìˆ˜ì§‘"""
        # Person ìƒì„± ë° ì‹œë®¬ë ˆì´ì…˜
        person = Person(self.params, rng)
        person.simulate()
        
        output = Output(self.params)
        
        # Episode ì‹¤í–‰
        transitions = []
        total_reward = 0.0
        num_screenings = 0
        cancer_detected = False
        
        current_age = self.min_age
        years_since_last = self.max_history
        
        while current_age < min(person.death_age, self.max_age):
            # State ìƒì„±
            state = self._get_state_vector(person, current_age, years_since_last)
            
            # Action ì„ íƒ (Random policy - PPO í•™ìŠµ ì „ì´ë¯€ë¡œ)
            action = self._select_action_random()
            
            # Action ìˆ˜í–‰
            if action == 1:  # Screen
                prev_cost = output.total_discounted_cost
                person._perform_colonoscopy(current_age, output, is_screening=True)
                step_cost = output.total_discounted_cost - prev_cost
                
                # Variantë³„ reward ê³„ì‚°
                reward = -(step_cost * 0.0001 + self.cost_penalty)
                
                # Aggressive: ì•” ë°œê²¬ ë³´ë„ˆìŠ¤
                if self.variant == 'aggressive' and output.screen_detections > 0:
                    reward += self.cancer_detection_bonus
                    cancer_detected = True
                
                num_screenings += 1
                years_since_last = 0
            else:
                reward = 0
                years_since_last = min(years_since_last + 1, self.max_history)
            
            # ìƒì¡´ ë³´ìƒ
            current_age += 1
            done = False
            
            if current_age >= person.death_age:
                reward -= 1.0  # ì‚¬ë§ í˜ë„í‹°
                done = True
            elif current_age >= self.max_age:
                reward += 1.0  # ì™„ì£¼ ë³´ìƒ
                done = True
            else:
                reward += self.survival_reward
            
            # Next state
            next_state = self._get_state_vector(person, current_age, years_since_last)
            
            # Transition ì €ì¥
            transitions.append({
                'state': state,
                'action': action,
                'reward': reward,
                'next_state': next_state,
                'done': done
            })
            
            total_reward += reward
            
            if done:
                break
        
        # Episode summary
        life_years_gained = min(current_age - self.min_age, person.death_age - self.min_age)
        
        return {
            'total_reward': total_reward,
            'life_years_gained': life_years_gained,
            'total_cost': output.total_discounted_cost,
            'cancer_detected': cancer_detected,
            'num_screenings': num_screenings,
            'transitions': transitions,
            'variant': self.variant
        }
    
    def _get_state_vector(self, person, age, history):
        """State vector ìƒì„±"""
        age_norm = age / 100.0
        hist_norm = history / 10.0
        gender = float(person.gender)
        zones = np.zeros(13)
        
        adv_threshold = getattr(self.params, 'adv_polyp_transition', 5)
        for p in person.polyps:
            if p.age_developed <= age < p.age_end:
                loc = min(p.location, 12)
                if p.stage >= adv_threshold:
                    zones[loc] = 2
                else:
                    zones[loc] = 1
        
        for c in person.cancers:
            if c.age_developed <= age:
                loc = min(c.location, 12)
                if c.detected:
                    zones[loc] = 4
                else:
                    zones[loc] = 3
        
        return np.array([age_norm, hist_norm, gender] + list(zones), dtype=np.float32)
    
    def _select_action_random(self):
        """Random action (PPO í•™ìŠµ ì „ baseline)"""
        return np.random.choice([0, 1])


# ==================== Main Collection Function ====================
def collect_diverse_dataset(params, episodes_per_variant=1000, save_path='diverse_ppo_data.pkl'):
    """
    3ê°€ì§€ PPO variantë¡œ diverse dataset ìˆ˜ì§‘
    
    Args:
        params: SimulationParameters
        episodes_per_variant: ê° variantë‹¹ episode ìˆ˜
        save_path: ì €ì¥ ê²½ë¡œ
    
    Returns:
        ì „ì²´ episode ë¦¬ìŠ¤íŠ¸
    """
    all_episodes = []
    
    for variant in ['conservative', 'balanced', 'aggressive']:
        collector = PPOVariantCollector(params, variant=variant)
        episodes = collector.collect_episodes(n_episodes=episodes_per_variant)
        all_episodes.extend(episodes)
    
    # ì €ì¥
    with open(save_path, 'wb') as f:
        pickle.dump(all_episodes, f)
    
    print(f"\nğŸ’¾ ì´ {len(all_episodes)} episodes ì €ì¥: {save_path}")
    
    return all_episodes


if __name__ == "__main__":
    from parameters import SimulationParameters
    
    params = SimulationParameters('settings.ini')
    
    # Diverse dataset ìˆ˜ì§‘ (ê° 1000 episodes)
    dataset = collect_diverse_dataset(
        params,
        episodes_per_variant=1000,
        save_path='diverse_ppo_data.pkl'
    )
    
    print("\nğŸ“Š Dataset í†µê³„:")
    print(f"  Conservative: {sum(1 for e in dataset if e['variant'] == 'conservative')}")
    print(f"  Balanced: {sum(1 for e in dataset if e['variant'] == 'balanced')}")
    print(f"  Aggressive: {sum(1 for e in dataset if e['variant'] == 'aggressive')}")
